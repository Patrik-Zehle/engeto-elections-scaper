"""
main.py: T콏et칤 projekt do Engeto Online Python Akademie (Elections Scraper)
author: Tvoje Jm칠no
email: tvuj.email@example.com
discord: tvojejmeno#1234
"""

import sys
import csv
import requests
from bs4 import BeautifulSoup
from typing import List, Dict


def over_argumenty() -> tuple[str, str]:
    """Zkontroluje a vr치t칤 argumenty z p콏칤kazov칠 콏치dky."""
    if len(sys.argv) != 3:
        print("Chyba: Program vy쬬duje 2 argumenty: <URL> <vystup.csv>")
        sys.exit(1)
    
    url, nazev = sys.argv[1], sys.argv[2]
    if not url.startswith("https://www.volby.cz"):
        print("Chyba: URL mus칤 b칳t z dom칠ny volby.cz")
        sys.exit(1)
    if not nazev.endswith(".csv"):
        print("Chyba: Soubor mus칤 m칤t p콏칤ponu .csv")
        sys.exit(1)
    return url, nazev


def ziskej_soup(url: str) -> BeautifulSoup:
    """St치hne str치nku a vr치t칤 BeautifulSoup objekt."""
    try:
        resp = requests.get(url)
        resp.raise_for_status()
        return BeautifulSoup(resp.text, "html.parser")
    except requests.exceptions.RequestException as e:
        print(f"Chyba p콏ipojen칤: {e}")
        sys.exit(1)


def ziskej_seznam_obci(url: str) -> List[Dict]:
    """Vr치t칤 seznam obc칤 (k칩d, n치zev, url detailu)."""
    soup = ziskej_soup(url)
    obce = []
    base = url.rsplit('/', 1)[0] + '/'
    
    # Hled치me v코echny tabulky s obcemi
    for t in soup.find_all("table", {"class": "table"}):
        for r in t.find_all("tr")[2:]:
            c_bunka = r.find("td", {"class": "cislo"})
            n_bunka = r.find("td", {"class": "overflow_name"})
            
            if c_bunka and n_bunka:
                link = c_bunka.find("a")
                if link:
                    obce.append({
                        "code": c_bunka.text.strip(),
                        "location": n_bunka.text.strip(),
                        "url": base + link["href"]
                    })
    return obce


def ziskej_detaily_obce(url: str) -> Dict:
    """St치hne po캜ty voli캜콢 a hlasy pro strany v konkr칠tn칤 obci."""
    soup = ziskej_soup(url)
    data = {}
    
    # 1. Z치kladn칤 statistiky (Voli캜i, Ob치lky, Platn칠 hlasy)
    # Hled치me podle hlavi캜ek (headers) v HTML tabulce - sa2, sa3, sa6
    try:
        data["registered"] = soup.find("td", headers="sa2").text.replace('\xa0', '')
        data["envelopes"] = soup.find("td", headers="sa3").text.replace('\xa0', '')
        data["valid"] = soup.find("td", headers="sa6").text.replace('\xa0', '')
    except AttributeError:
        print(f"Chyba p콏i parsov치n칤 detail콢: {url}")
    
    # 2. Hlasy pro jednotliv칠 strany
    # Strany jsou ve v칤ce tabulk치ch, projdeme je v코echny
    tables = soup.find_all("table", {"class": "table"})
    for table in tables:
        for row in table.find_all("tr")[2:]:
            party = row.find("td", {"class": "overflow_name"})
            if party:
                # Hlasy jsou obvykle v n치sleduj칤c칤ch bu켿k치ch
                # Hled치me bu켿ku s 캜칤slem (hlasy) - indexy se mohou li코it, hled치me tu spr치vnou
                cols = row.find_all("td")
                # Hlasy b칳vaj칤 ve 2. nebo 3. sloupci s 캜칤sly
                for col in cols:
                    if col.get("headers") and ("t1sa2" in col["headers"] or "t2sa2" in col["headers"]):
                        data[party.text.strip()] = col.text.replace('\xa0', '')
                        break
    return data


def uloz_do_csv(data: List[Dict], soubor: str):
    """Ulo쮂 seznam slovn칤k콢 do CSV."""
    print(f"Ukl치d치m data do souboru: {soubor}")
    if not data:
        return

    # Z칤sk치me hlavi캜ku ze v코ech kl칤캜콢 prvn칤ho z치znamu (v캜etn캩 stran)
    fieldnames = data[0].keys()
    
    with open(soubor, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=";")
        writer.writeheader()
        writer.writerows(data)


def main():
    url_okresu, soubor = over_argumenty()
    
    print(f"STAHUJI DATA Z: {url_okresu}")
    seznam_obci = ziskej_seznam_obci(url_okresu)
    print(f"Nalezeno {len(seznam_obci)} obc칤. Zahajuji stahov치n칤 detail콢...")
    
    vysledna_data = []
    
    for i, obec in enumerate(seznam_obci, 1):
        # V칳pis pr콢b캩hu, aby u쬴vatel vid캩l, 쬰 se n캩co d캩je
        print(f"Zpracov치v치m ({i}/{len(seznam_obci)}): {obec['location']}")
        
        # St치hneme detaily (voli캜i, strany...)
        detaily = ziskej_detaily_obce(obec["url"])
        
        # Spoj칤me z치kladn칤 info (k칩d, jm칠no) s detaily
        komplet_obec = {**obec, **detaily}
        
        # Odstran칤me URL z fin치ln칤ho v칳stupu (nen칤 v zad치n칤 CSV)
        del komplet_obec["url"]
        
        vysledna_data.append(komplet_obec)
    
    uloz_do_csv(vysledna_data, soubor)
    print("HOTOVO! 游꿀")


if __name__ == "__main__":
    main()